# ğŸ˜ Github_Explainer

A chat bot that will explain all the function which is used in that repo file given ,powered by **LangChain, Hugging Face embeddings, and Google Gemini LLM**.  
The chatbot can analyze source code, process queries, and provide contextual answers using **Generative AI**.

---

## ğŸš€ Features
- ğŸ’¬ Interactive chatbot interface (Streamlit frontend).  
- ğŸ” Source code analysis: Upload GitHub repo link and ask questions about the codebase.  
- ğŸ§  Embeddings: Uses **Hugging Face Sentence Transformers (all-MiniLM-L6-v2)** for vector embeddings.  
- ğŸ¤– LLM Integration: Powered by **Google Gemini API** via LangChain.  
- ğŸ“š RAG (Retrieval-Augmented Generation) pipeline for contextual answers.  
- âš¡ Runs locally with simple setup.

---

## ğŸ› ï¸ Tech Stack
- **Frontend:** Streamlit  
- **Backend:** Python, LangChain  
- **Embeddings:** Hugging Face (sentence-transformers/all-MiniLM-L6-v2)  
- **LLM:** Google Gemini API  
- **Vector DB:** FAISS  

---
## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/entbappy/End-to-end-Medical-Chatbot-Generative-AI.git
cd End-to-end-Medical-Chatbot-Generative-AI
```

### 2ï¸âƒ£ Create Conda Environment
```bash
conda create -n llmapp python=3.10 -y
conda activate llmapp
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables  
Create a `.env` file in the root directory:
```
GOOGLE_API_KEY=your_gemini_api_key
```

### 5ï¸âƒ£ Run the App
```bash
streamlit run app.py
```

---

## ğŸ§‘â€ğŸ’» Usage
1. Enter the **GitHub repository link** in the input box.  
2. The app will clone the repo, parse the source code, and generate embeddings.  
3. Ask natural language questions about the codebase (e.g.,  
   *"What does the `download_hugging_face_embeddings` function do?"*).  
4. Get detailed AI-powered responses in real-time.

---

## ğŸ“¸ Demo
Hereâ€™s a screenshot of the chatbot in action:  

![Demo Screenshot](qde.PNG)

---

