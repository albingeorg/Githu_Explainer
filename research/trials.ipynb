{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d3754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (6.30.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (8.37.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (27.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (4.14.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\albin\\anaconda3\\envs\\llmapp\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec llmapp in C:\\Users\\Albin\\AppData\\Roaming\\jupyter\\kernels\\llmapp\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "!python -m ipykernel install --user --name=llmapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4602152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d238e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Gemini imports\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a7b029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Albin\\\\Desktop\\\\LLM_PROJECT\\\\research'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58abc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9ed8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\n",
    "    \"https://github.com/entbappy/End-to-end-Medical-Chatbot-Generative-AI\",\n",
    "    to_path=repo_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a751e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b04792",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7841432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Bappy Ahmed',\\n    author_email= 'entbappy73@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\n\\n\\nextracted_data=load_pdf_file(data=\\'Data/\\')\\ntext_chunks=text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, \\n    metric=\"cosine\", \\n    spec=ServerlessSpec(\\n        cloud=\"aws\", \\n        region=\"us-east-1\"\\n    ) \\n) \\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings, \\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n#Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\n#Download the Embeddings from HuggingFace \\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')  #this model return 384 dimensions\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\n\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ee3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b638183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a8bd30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = documents_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c77c01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76b62ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albin\\AppData\\Local\\Temp\\ipykernel_3460\\4030443929.py:4: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory=\"./db\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7303b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce586839",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8}),\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ab7232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings function?\"\n",
    "result = qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4873e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `download_hugging_face_embeddings` function downloads sentence embeddings from Hugging Face using the `sentence-transformers/all-MiniLM-L6-v2` model.  The function returns these embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60b7a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_pdf_file` function uses the `DirectoryLoader` from the Langchain library to load PDF files from a specified directory.  It returns a list of `Document` objects, each representing a loaded PDF file.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "question = \"what is load_pdf_file function?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
